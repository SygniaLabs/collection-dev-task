# ðŸ“‹ Interviewer Guide â€” Pipeline Challenge

> **âš ï¸ INTERNAL DOCUMENT â€” Do NOT include in the candidate's repo.**
> Remove this file before pushing to the candidate-facing repository.

---

## Interview Structure (1.5 hours total)

| Phase | Duration | Description |
|-------|----------|-------------|
| **1. Intro & Background** | 15 min | Team overview, role expectations, candidate experience deep-dive |
| **2. Exercise Briefing** | 5 min | Share Codespace/repo link, walk through the naive implementation together |
| **3. Bottleneck Analysis** | 5â€“10 min | Candidate reads code, identifies problems, discusses optimization strategy |
| **4. Hands-on Implementation** | 25â€“30 min | Candidate implements optimizations, runs benchmarks, explains trade-offs |
| **5. Architecture Discussion** | 15â€“20 min | Production scaling, cost analysis, new log types, monitoring, what-ifs |
| **Buffer** | ~10 min | Setup issues, overflow, wrap-up |

**Note:** The hands-on coding is intentionally ~30 minutes. With intro, setup potential issues (Codespaces boot, network, etc.), and discussion, this fills the 1.5 hour slot. The 50K line dataset is sized to complete processing in 1-5 minutes even on slower machines.

### Phase 2 Tips
- Walk through `pipeline.py` together â€” show them the Reader and Processor
- Point out `benchmark.py` and `sample_queries.sql` â€” these define success
- Don't give hints yet; let them identify the issues themselves in Phase 3

### Phase 3 Tips
- Ask: *"What bottlenecks do you see in this code?"*
- A strong candidate should identify 3-5 issues within a few minutes
- Ask them to prioritize: *"If you had time for only one optimization, which would have the biggest impact?"*

### Phase 4 Tips
- Let them work. Observe their process â€” do they plan first or dive in?
- If they're stuck after 10 min, you can hint: *"What if the Reader sent messages in batches?"*
- Ask them to run the benchmark periodically to see improvement
- If they finish batching quickly, prompt: *"What about the database schema?"*
- Don't worry if they don't finish everything â€” we're evaluating approach, not completion

---

## Optimization Levels â€” Scoring Guide

### Level 1 â€” Batching (60% of candidates should reach this) âœ… MUST-HAVE

**What to look for:**

1. **Redis batch writes** â€” Use `r.pipeline()` to batch LPUSH commands
   ```python
   pipe = r.pipeline()
   for msg in batch:
       pipe.lpush(QUEUE_NAME, msg)
   pipe.execute()
   ```

2. **Redis batch reads** â€” Pop multiple messages at once (Lua script, pipeline RPOP, or LRANGE+LTRIM)
   ```python
   pipe = r.pipeline()
   for _ in range(BATCH_SIZE):
       pipe.rpop(QUEUE_NAME)
   results = pipe.execute()
   ```

3. **PostgreSQL bulk inserts** â€” Use `executemany()` or `psycopg2.extras.execute_values()` with single COMMIT per batch
   ```python
   from psycopg2.extras import execute_values
   execute_values(cur, "INSERT INTO logs (...) VALUES %s", batch_rows)
   conn.commit()
   ```

**Expected throughput improvement:** 5â€“10x

**Key talking points to listen for:**
- "Each LPUSH is a TCP round-trip; batching amortizes the network overhead"
- "Committing per INSERT forces a WAL fsync per row â€” bulk commit is critical"
- "On AWS, SQS charges per API request. Batching 10 messages per request = 10x cost reduction"

---

### Level 2 â€” Message Compression (40% of candidates)

**What to look for:**
- Compress batch payloads with gzip before publishing to Redis
- Send one Redis message containing N compressed log lines instead of N individual messages
- Understanding of SQS message size limits (256KB) and pricing model

**Key talking points:**
- "A batch of 100 JSON lines gzip-compressed is ~10x smaller than uncompressed"
- "Fewer, larger messages = dramatically lower queue costs"
- "This is a direct trade-off: compression CPU cost vs. network/queue savings"

---

### Level 3 â€” Schema Design & Database Indexing (35% of candidates) â­ DIFFERENTIATOR

**What to look for:**

1. **Separate tables per log type** with proper typed columns:
   ```sql
   CREATE TABLE firewall_logs (
       id BIGSERIAL PRIMARY KEY,
       timestamp TIMESTAMPTZ NOT NULL,
       src_ip INET NOT NULL,
       dst_ip INET NOT NULL,
       action VARCHAR(10),
       protocol VARCHAR(10),
       src_port INTEGER,
       dst_port INTEGER,
       bytes_sent INTEGER,
       rule VARCHAR(50),
       source_file VARCHAR(255)
   );
   ```

2. **B-tree indexes** on commonly queried fields:
   ```sql
   CREATE INDEX idx_fw_src_ts ON firewall_logs (src_ip, timestamp);
   CREATE INDEX idx_fw_dst_port ON firewall_logs (dst_port);
   CREATE INDEX idx_dns_domain ON dns_logs (query_domain);
   CREATE INDEX idx_auth_status_ip ON auth_logs (status, source_ip);
   ```

3. **Awareness of INET type** for IP columns (enables range/subnet queries)
4. **Discussion of timestamp partitioning** for large tables

**Key talking points:**
- "JSONB querying with `->>'field'` can't use B-tree indexes effectively"
- "Typed columns give better storage efficiency and query planning"
- "Separate tables mean each can be indexed for its specific query patterns"
- "In production at scale, this is why systems use columnar stores like Snowflake"

---

### Level 4 â€” Architecture & Concurrency (25% of candidates) â­â­ SENIOR SIGNAL

**What to look for:**

1. **Split Processor into Parser + Indexer** (independent scaling)
2. **Multiple consumer instances** (N parallel processors)
3. **Per-log-type routing** â€” reader classifies lines, routes to type-specific queues
4. **Thread pool or multiprocessing** for parallelism

**Target architecture:**
```
Files â†’ [Reader] â†’ redis:raw_queue
                       â”œâ”€â†’ redis:firewall_queue â†’ [FW Parser]  â”€â”€â†’ redis:index_queue â†’ [Indexer] â†’ PG
                       â”œâ”€â†’ redis:dns_queue      â†’ [DNS Parser] â”€â”˜
                       â””â”€â†’ redis:auth_queue     â†’ [Auth Parser]â”€â”˜
```

**Key talking points:**
- "Parsing is CPU-bound, indexing is I/O-bound â€” they should scale independently"
- "Adding a new log type just means adding a new parser + queue; the indexer is generic"
- "In k8s, we'd autoscale parsers based on queue depth (HPA)"
- "This maps to SQS â†’ Lambda or SQS â†’ ECS tasks in AWS"

---

### Level 5 â€” Production Readiness (15% of candidates) â­â­â­ EXCEPTIONAL

**What to look for:**
- **Dead letter queue** for unparseable/failed messages
- **Backpressure** â€” reader slows down when queue is too deep
- **Idempotency** â€” handle duplicate messages gracefully (dedup by hash)
- **Dynamic log type registration** â€” parser registry / plugin pattern
- **Configurable batch size with timeout** â€” "flush after N messages OR T seconds, whichever comes first"
- **Monitoring** â€” queue depth, processing latency, error rate, throughput metrics
- **Error recovery** â€” what happens if PG goes down mid-batch?

---

## Benchmark Interpretation â€” Percentage-Based Scoring

**âš ï¸ Do NOT use absolute throughput thresholds.** Machine performance varies dramatically:
- GitHub Codespaces (2-core VM): ~500-2,000 rec/sec baseline
- Local laptop (8-core): ~2,000-8,000 rec/sec baseline  
- M3 Max (12-core): ~5,000-15,000 rec/sec baseline

### What to Evaluate

1. **Relative improvement** â€” Did they achieve meaningful speedup?
2. **Code quality** â€” Is the implementation clean, configurable, maintainable?
3. **Understanding** â€” Can they explain *why* each optimization helps and estimate cost impact?

### Typical Improvement Ranges (50K records)

| Implementation Level | Expected Speedup | Baseline Time | Optimized Time |
|---------------------|------------------|---------------|----------------|
| **Naive (baseline)** | 1x | 30-120 sec | â€” |
| **+ Basic Batching** | **5-10x** | 30-120 sec | 3-15 sec |
| **+ Schema + Indexes** | **10-20x** | 30-120 sec | 2-6 sec |
| **+ Compression** | **15-25x** | 30-120 sec | 1-4 sec |
| **+ Concurrency** | **20-40x** | 30-120 sec | 1-3 sec |

**What matters:** Did they move down the table? Not the exact seconds.

### Red Flags
- No measurable improvement after 20 minutes of work
- Can't explain why their changes should help
- Breaks correctness (benchmark queries fail)
- Overly complex solution for minimal gain

### Green Flags
- 5x+ improvement with clean batching implementation
- Discusses trade-offs (batch size vs. latency, memory, etc.)
- Mentions cost implications (SQS pricing, compute, storage)
- Suggests monitoring/observability improvements

---

## Discussion Questions (Phase 5)

Use these AFTER the hands-on portion. Pick 3â€“4 based on what the candidate covered:

### Scaling
> *"A new customer sends 10x the log volume. What changes?"*

Expected: Horizontal scaling of consumers, queue partitioning, batch size tuning, possibly streaming (Kafka) instead of polling, auto-scaling based on queue depth.

### Schema Evolution
> *"A new log format has a field we've never seen before. How do you handle schema changes?"*

Expected: Additive-only migrations, backward compatibility, Snowflake VARIANT type, schema versioning, graceful degradation for unknown fields.

### Low-Latency Queries
> *"Analysts want sub-second query response for the last 24 hours of firewall data. How?"*

Expected: Hot/cold storage tiers, in-memory caching, materialized views, read replicas, Elasticsearch for recent data, or time-based partitioning with partition pruning.

### Cost Optimization
> *"Our SQS bill spiked. How do you investigate and reduce it?"*

Expected: Batch API calls (up to 10 per request), message compression, reduce empty receives (long polling), analyze CloudWatch metrics for queue patterns, possibly switch to SNS fan-out.

### Failure Handling
> *"A parser bug produced malformed data for 2 hours. How do you recover without data loss?"*

Expected: Dead letter queue, replayability from source files, bookkeeping/integrity tracking, versioned processing, ability to re-process a time window.

### Monitoring
> *"What metrics would you add to this pipeline for production observability?"*

Expected: Queue depth over time, processing latency (end-to-end and per-stage), error rate by type, throughput (records/sec), batch sizes, memory/CPU utilization, data lag (newest record age).

---

## Scoring Matrix

| Area | Weight | 1-2 (Junior) | 3-4 (Mid) | 5 (Senior) |
|------|--------|--------------|-----------|------------|
| **Bottleneck Identification** | 15% | Spots 1-2 issues | Systematically finds most issues | Immediately sees all issues, prioritizes by ROI |
| **Batching** | 20% | Basic batching in one place | Batching across reader + processor + DB | Configurable batch size with timeout fallback |
| **Schema Design** | 20% | Keeps single table, maybe adds an index | Separate tables with proper types | Typed tables + composite indexes + partitioning discussion |
| **Architecture Thinking** | 20% | Single-process improvements | Discusses component splitting and scaling | Draws production architecture with queue routing & auto-scaling |
| **Cost Awareness** | 10% | "Batching is cheaper" | Cites specific SQS pricing, estimates savings | Full cost model, compares optimization ROI |
| **Communication** | 15% | Explains what they're doing | Explains why, discusses trade-offs | Proactively raises edge cases, asks clarifying questions |

### Scoring Thresholds

| Score | Verdict |
|-------|---------|
| **4.0 â€“ 5.0** | Strong hire â€” senior-level pipeline thinking |
| **3.0 â€“ 3.9** | Hire â€” solid fundamentals, can grow into the role |
| **2.0 â€“ 2.9** | Borderline â€” good coder but lacks scale/systems thinking |
| **< 2.0** | No hire â€” insufficient depth for this role |

---

## How This Maps to Our Real Pipeline

| Exercise Component | Our Pipeline |
|---|---|
| Reader â†’ Redis LPUSH | **Dealer** â†’ SQS â†’ **Reader** with `GzipCompressingPublisher` |
| Redis list queue | **RabbitMQ / SQS** pipeline queues |
| Processor parsing | **Windmill Parser** with transport/source parsing chain |
| Log type routing | **Parsing chain** detection â†’ per-source parsers (Check Point, ESXi, etc.) |
| PostgreSQL indexing | **Indexer** â†’ Elasticsearch + Snowflake via `SnowflakePublisher` |
| Batch size + timeout | **Batcher** class (batch_size_messages, batch_size_bytes, timeout) |
| Dead letter queue | **Retry managers** in Enricher |
| Progress tracking | **Bookkeeper** tracking file processing integrity in PostgreSQL |

A candidate who excels at this exercise will naturally understand and contribute to our pipeline from day one.

---

## Pre-Interview Checklist

- [ ] Push repo to `SygniaLabs/pipeline-challenge` (REMOVE this file first!)
- [ ] Test the Codespace setup works (create one, run the benchmark)
- [ ] For each candidate: create a branch `candidate/<name>-<date>`
- [ ] Send the candidate the Codespace link (or repo URL for local setup)
- [ ] Have the scoring matrix open during the interview

